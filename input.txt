All right. Welcome back, everyone. How's it going? Getting through it. All right. Midway through week two now, so I guess, I don't know, maybe next week we'll be hitting our stride and getting more in the swing of things. But yeah, the plan for today is to talk about things that are called embarrassingly parallel and things that are not. So. Yeah, that's that's kind of the plan and will understand what that word means. I don't know. It's kind of a fun phrase. And yeah, let's jump into it. But first a couple of announcements. First of all, a homework assignment has been posted. I had originally hoped that everything would be set up with this HPC clusters. So this is the high-performance computing cluster that Amherst College just acquired. And it's literally like just this semester getting up and running. And it'll be a fun resource that will have access to going forward in the class. But not everything is set up with it yet in terms of the documentation. So ignore everything on assignment one that asks you to compare the performance of your program on your computer to the performance on the HPC cluster, because I don't know that everything will be ready to go in the next day. The state of where things are with the cluster right now is that you should all have accounts. I don't think that you would have gotten a notification that you have an account, but you will have an account on this machine. So the documentation that I'm waiting on slash putting together is how you get access to it from your computer. So you can look, if you want to read about the cluster, you can go to this website, HPC dot Amherst dot edu. And it'll tell you a little bit about the cluster, what its capabilities are, all of that stuff. There's some documentation for it, but specifically what's missing from the documentation is how to access it from the terminal or from the shell. And that's exactly what we're going to need for this class. But if you already are comfortable doing remote login, doing SSH and all of that stuff. You can log into it this way. So you can use your Amherst ID, so like your email handle. So if you do SSH Amherst ID at HPC diameters dot edu, you'll login to the machine and you'll have access to a state-of-the-art supercomputers that we now own. So that's pretty cool. And hopefully by next week, everything will be together that I'll have tutorials and stuff to help you learn how to use it. And then we'll be able to have this shared resource that we can use to benchmark our programs and see, um, how parallelism really helps. Even more when you have this supercomputer at your disposal than just using your laptop, although you will already see a huge performance boost just in your laptop depending on what task we're looking at. So that's the state of things with the first programming assignment. So you'll have a fairly short written assignment due next week. So I'll post this over the weekend. And it'll just be two or three questions. Conceptual questions about stuff that we've been talking about in lecture. And it'll have a question or two related to some of the readings as well. So keep an eye out for that. Then also office hours. So I think my schedule is more or less finalized now. And also the TA for the course, Mary Kay, will be having drop-in office hours. So she offered Wednesday evening seven to 09:00 P.M. and these will be just in the computer labs. So like down at the end of this hallway here in seed 109, there's like a computer lab that we don't we don't usually use it too much for courses, but it's available for office hours. So that's another resource for you. And then I'm going to have my individual office hours by appointment On Thursdays 1-230. This afternoon. I'll I'll post a sign-up sheet so you can sign up for a 15-minute one-on-one slot with me. And those can be either in person or on Zoom depending on your availability and all of that. So cool. I think that's it for administrative stuff. Any questions administrative wise before we jump into new material? For what? Yeah. Yeah. So those will be like drop-in office hours after class as well. So that's those, those will all be the same. Cool. Awesome. Alright, so yeah, let's talk outline for the day. So we'll have go over this activity that I handed out at the end of class on Friday. We can talk about that. Oh, by the way, you can set up a thing so you could submit this to Moodle. If you also wrote some workout by hand. You can give that to me whenever, during lecture now. Yes, so we'll talk about that. I wanted to talk a little bit more about the difference between parallelism and concurrency. Because they think that this is a distinction that is, well, it's something that's worth understanding the differences between these ideas and how they relate to each other and which ones introduce problems and which ones don't. We'll talk about embarrassingly parallel computation. And then we'll talk about limits of parallelism. So that'll be what we worked through today. So let's talk about this activity from the last lecture. We had this method that was just increment. So what does it do? It takes an array called a, and it just, each method goes through an increments the value of the array at each index. So we just have a while loop. So this is just increment the value of the array. Then we just have this variable i that keeps track of where we are. And so there was this, note that this variable a, this is shared between threads. This is an object that's being concurrently modified by more than one process, but then the other variables that are not arguments to this method. So in particular I, this is what's called a thread local variable. When you write a multi-threaded program in Java and you write your run method, every variable that you declare inside of the method is local to that thread. If you have two threads, both running this code, they both have a variable called i that is defined inside of the method. They each have their own variable, ie. There's no communication between multiple threads for anything declared inside of the thread. Inside of its definition. The only way that threads can communicate with each other and interact with each other is if you explicitly give them a shared object. So in this case, the shared object is a, this, this array that's being passed into as an argument, this increment method. So it's the same array being modified by multiple threads. But all of the surrounding code, like the keeping track of the integer I, doing the logic of the while loop, all of that, that is happening independently for each thread. There's no crosstalk between them. So this is a method. We're just taking this array and we're incrementing each value of it, whatever was there previously, we want to increment that value by one. The first question was, let's say I give it an array with four zeros in it. And we have two threads that are concurrently calling this, this increment method. So the question is, what are the possible outcomes? What are the possible value that I could get? Four possible, I guess, sequence of four values that I could get from this array a. So, yeah, Who wants to start us off? What is a possible outcome? But if I run this code with two threads that I could see. Yeah, cool. So we get star, star, star, star. So we get an r array is, is a four values again. And each star could be one or two. Let's just say like, let's, let's take things a little bit closer and look just at this first value. So how could it be that this value would end up being one when both threads complete? What would the execution B? Yeah. Uh-huh. Nice. So they both, if both threads are doing exactly the same thing at exactly the same time. So they both start at I equals zero. They both enter the while loop at the same time, and they both hit this first instruction at the same time. Ai equals Ai plus one. Then we have to be really careful about what's going on here because what is actually happening with each thread wants reading the value a of I, it's adding one to it. And then it's writing the new value. So we have one read and one right, with the shared memory. So we can have, right, So if we had, let's make a little diagram here. So here's thread one, thread two. So maybe we have the following thing. So in the very first kind of logical step, T1 reads a of one, a of zero. That's the first thing that happens. And then the next step is that T2 reads a of zero, a of zero. So now they both see the value zero because that was the original value. And then they, they both incremented. So increment. Then maybe this one increments. So now they both read this value zero. They both incremented the value zero. So then at the end, at this stage they both are storing the value one. And then they both right one. So maybe the first thread writes first, so right one. And then right one. Okay? So if we did everything in this sequence, we would get one is written by both of the threads. So one of them wins. It doesn't matter which 11. But either way, no matter which one wins, the value that gets written as one. This is one possibility. So how can it be the case that I end up with the value two? Yeah. Oh yeah. Yeah, things are either not. So we're each, so each thread is only incrementing each index once. It's time to find out through one. So after one iteration, each thread will have access to every value in the array exactly once. So if it started at zeros, if, say thread one comes along and does all of its actions. So if we just had one thread, what would the array b after that? After just one thread did its thing. Well, so thread one comes along and it does all of these, does this whole while loop. The first iteration of the while loop increments the first value of the array. The second increments, the second value, the third increments the third value, the fourth increments the fourth value. After one thread. Does these instructions. The array is 1111. Then if the second array comes along or the second thread comes along and does the same action after the first thread is finished. The second thread comes along and increments all of the values again. So it increments the first value to it increments the second value to two, it increments the third value to two and so on. Sure. Yeah, that could be the other option. So this is I guess I explained already how we can get to them. So this would give us one. If we have this execution to get to e.g. we could have the first thread reads a of zero increments and then writes. So then the value becomes one. Then the second thread reads the one increments and then writes so two. Then we would end up with a two. And the idea is that because I don't know how the individual steps are interleaved with one another. If I could get a value of one or two for the first index, I could get a value of one or two for the second index and so on, any combination of these values as possible. So this is a really dangerous piece of code. Like this is something that, you know, it, it looks totally fine from the perspective of writing a single-threaded program. But there's a huge amount of non-determinism see in this, even though there's only four steps that are being performed, the possible number of outcomes of this computation for two threads is two to the fourth power. So there's whatever that is, 16 different possible outcomes, even though I have this very simple code. So this is one of the challenges. Again, reasoning about concurrent programs is that is that I have many possible outcomes that I need to reason about. And maybe I don't want to have this much non-deterministically and what my program is actually doing. Cool. So any questions about this example? Let's jump to the next one. So now instead of just two threads, Let's say I have k threads. So ten threads, 20 threads, 400 threads. What are the, how many possible outcomes are there? What are the possible values that I could get? Yeah. Yeah, so each index could store any value from one to K. So how could I get, let's say K is ten. How can I get a value of six? What could my execution looked like in the first, in the first a of zero. Check that happen. Yeah. Okay. Nice. Yeah. So I guess I can get up to six by just having the first six, or I guess maybe we want to have the first five threads read and write in sequence. So the first thread reads and writes. The second thread reads and writes, The third thread reads and writes. So let's see, first five threads read increment right? In succession. Okay, so the first one finishes its thing, the second one finishes its thing, and so on. So now the value stored is five. Then the next five threads all read. Last five threads all read. They all read the value five. And then the last five threads, all right? I guess they read and then increment. Then the last five threads will all right the value six. There's nothing special about K equals ten and value equals six, right? Like we can make this same sort of the same sort of behavior for any value of k, for any, for any numerical value that we want to get between one and k. So why couldn't it be the case that we would ever get a zero in one of these? Yeah. Uh-huh. Yeah. So no matter what, when I read I read a value that's at least zero, right? And no matter what, when I write, I write a value that's one bigger than what I read. So if the smallest value that I could read is zero, the smallest value that I could write as one. No thread is ever going to write zero. But again, this is something that like in this case, I think it's it's fairly simple to reason about. But things get complicated, things get settled very quickly. So it's important to like, check every every step of the way. Like, why is it the case that I couldn't get a different answer? You have to convince yourself that and that's what we're going to spend a substantial amount of time next week doing when we talk about mutual exclusion, is trying to reason about all possible executions of programs that are not too much more complicated than this. But we'll see that they're immensely subtle. So is everyone happy with these examples? Any questions? Alright, awesome. Yeah. So the last five grades are reading and then writing. How is it that the right ovaries? They will? And this was what the model of computation that we talked about on Friday is that we have these atomic operations of read and write. But when you write something to a memory location and somebody else writes after you, they overwrite what you wrote, and you have no way of knowing that as a threat unless you try to read again later. But you don't have control over whether you read before or after they wrote. So you still don't know, like there's always this uncertainty about what the true state of the computer's memory is. No one ever has a like a, a complete view of the state of the system at any given time. Because they're not checking that. Only Exactly, yeah. Yeah, it was there another question? Yeah. Yeah. Right. So this is from the RAM model that we talked about before. We have what's going on in memory. And that's where a is stored. Then we have our CPU that has a few cores on it. So each of these is a core. So each core reads the memory value and the increment happens in the CPU. So it's some register in the CPU that's not part of the shared memory, that reads the value a or whatever it is, and then increments it so it stores it to a register in the CPU. And each thread, Each, each core has its own version of what it reads, what it increments, and then what it's going to write. And then there's this interaction where it actually writes the value back to the shared memory, The incrementation. So there's kind of two different places of memory. There's the shared memory that stores the value of the array. Then there's the individual registers in the cores store the US very small number of values that are currently being operated on. So yeah, there is a set that, right like this. These are where there's no crosstalk between the registers of the CPU. It's that each thread has its own, has full access to all of the registers associated with that core. Cool, Awesome. Great. So yeah, parallelism versus concurrency. So again, two words that I'll say a lot. And so I wanted to just give one more kind of sense of what the difference between these are. So concurrency means that I perform multiple tasks that occupy overlapping time intervals. So if I look at a schedule of myself for a semester. So we started what On January 30th. And when does the semester and like May 15th or something like that. Let's say it's May 15th. I look at what I'm teaching over that time. So over that time, I am teaching this course to 73. I'm also teaching this visualization course to 25. So in some sense, right, like I'm teaching these courses at the same time because I'm teaching them in the same semester. But if I look at my actual schedule of what I'm doing when I am doing work. I'm not working at the same like I'm not making progress towards working on both of these classes at the same instant. If I look at my day schedule. So here's Wednesday. This is me. This is well, So 10-1050. This time is blocked out four to 73. And then I have a little break. And then 2-320 this time is blocked out for and I should use a different color. I guess. This time is blocked out for 225. So in terms of my resource, my own resources, I am, I might have different threads, right? Like I have different pieces of different directories on my computer that I modify different sets of slides for these different courses. But at any given instant of time, I am only actively making progress on one of these courses at a time. These courses are being offered by me concurrently because they start and finish at the same times and those time intervals overlap. But I'm not working on these classes in parallel. Because at any given instance, I am only performing operations on one, for one of these classes at a time. So this is concurrency is that I have two classes that occupy the same interval of my time of my life. But at any given instance, my schedule only allows me to be teaching one class. I can only be physically present in a single classroom. I am one process operating on two threads. I have a thread for this course, I have a thread for my other course. So even though there's only one process that is active me, I am I'm operating on these courses concurrently. I haven't I haven't finished teaching to 25 when I'm teaching 273 or vice versa. That's what we mean when we say concurrency, is that I have two programs that are running at with overlapping intervals. On the other hand, we could have parallelism. Parallelism is the ability to make progress on multiple tasks at precisely the same time. E.g. this course, I know conflicts with Galois theory math courses being offered. I know that because one of you asked if I could move this class. But yeah. So these two classes are being taught at the same time slot by two different professors, right? So we have, if I look at Wednesday, so here's Wednesday. And I look at the ten to 11 slot. Here is computer science to 73. And here is math. For ten. We have two professors to processors are two processes that are working on these tasks at the same time. Both of these courses are being offered at the same time. This is an example of course, there's, of course is being offered in parallel. So again, two different courses, but they are actually being taught. There's progress being made towards both of them at the same time. And that's an example of parallelism. So if we have two professors, we can teach twice as many courses at precisely the same time. So whenever we have a parallel execution of something, it needs to be a concurrent execution as well. Because we can be doing these multiple tasks at the same time. But a concurrent execution doesn't have to be done in parallel. Concurrent execution just means that I have multiple programs running at the same time that haven't both where one doesn't complete before the other starts. Yeah. So there's like I would say their schedules can interleave is the way that the the I guess the word that gets thrown out is that you have these interleaving pieces of the execution. So I say work on this thread for a little while, then work on this thread, then go back to the first thread, then go back to the second thread. And the exact order in which these, these, these operations are interleaved with one another is not determined by the programmer. Again, in this case, it's determined by the operating system or whoever is executing the threads. So it wouldn't be entirely right? Yeah, I mean, like you can say, right. So maybe that's one way of saying it, right? Like there's a discontinuity and like I perform some operations and then I stopped for a while and let let somebody else do their operations. Yeah, Sure, Cool. Awesome. So right, so this is the difference between concurrency and parallelism, right? So why, why do we want to reason about both of these? Why are these related topics so, well, we have this one relation that a parallel execution is automatically a concurrent execution. But we can have concurrency without parallelism. But in terms of why we like parallelism, parallelism can give a performance boost, right? So if I can do more than one thing at a time, if I can make progress on more than one task at a time, I can perform all the tasks that I need to do in less time overall, I can decrease the latency of performing these tasks. And that's one of the big, one of the big focuses of this class. But on the other hand, concurrency, like you probably haven't done concurrent program or I don't assume that you've seen concurrent programming before this class. But it's necessary for like, really basic functionality in a computer. Like even if you think about it, what does a computer do? So it has an operating system. The operating system manages the computer's resources and it tells which programs to run. There's a separate program that monitors the track pad on my computer. There's a separate program that displays information on the screen of my computer. There's a separate program that allocates resources for other programs. Like at any given time, I've many programs running concurrently on my computer. Even before computers had parallel hardware, it was possible to do that by doing exactly what we do with our own lives and scheduling things. Which is, you block some resources to work on one class, then you block some resources to work on another class and so on. And so even if we, even if we don't want to get this performance boost, that is how I'm trying to sell this material to you. It's still necessary to reason about concurrency at the level of operating systems. But again, unless you've taken a course on operating systems, you wouldn't necessarily have ever encountered. I'm like concurrency at this level because it tends to be that the operating system gives you the illusion that what you're doing when you write a single threaded or an ordinary program. It gives you the illusion that everything is happening in one go and that you have complete access to the computers resources. So concurrency is something that we need to deal with independent of trying to get the best performance out of parallel hardware. And so all of the issues that we've seen with nondeterminism are already there for concurrent programs. So these are not problems that are introduced by having multi-core processors. These are problems that are fundamental to computer science. At the operating system level, at the architectural level, we need to deal with concurrency. We need to be correct about it. Because the issues of correctness of these programs again, will fundamentally say whether or not our program is successful under all possible executions. So we have this great thing. Parallelism gives us extra performance. We have this necessary thing that concurrency is inherent to all interesting computer systems. And then we have this challenge. Nondeterminism makes things hard to reason about. Yeah. As far as we're going in this class, we, we're at the mercy of the operating system. Operating systems nowadays are very, very well tuned and optimized to deal with multi-threaded programs. And so hopefully that's what you'll see in the first, in the first lab is like if your computer has four cores, you will see almost exactly a fourfold increase in throughput if your computer, if you make four threads. And that's because again, the operating systems tend to be very good. They like, they're trying to squeeze out as much performance as they possibly can from your processor. And so it's incredibly wasteful to not do things in parallel if, if you, if you think you can. Cool. Well, yeah, again, it's not something we can control. It's just something that we can see what happens. So that's what I added at one of the things that's kinda fun in doing the labs for this course I think will be the, you know, you'll, you'll have a great idea of some way that you think will solve a problem faster. And then you do an experiment and say, nope, that's not what the operating system does or maybe it was already making some optimization under the hood that you were unaware of. And it's optimisation was better than the one that you've coded. So there's a lot of kind of tinkering, trial and error and stuff like that in the in the labs. Alright. So yeah, let's go back to where we were before. This was, had, the first experiment that we ran was this multithreaded counter. So I gave this counter object to a bunch of threads on my computer. You all did that. And when you said increment the counter 100 million times in total, so 25 million times each from four threads. And you got a final count that was somewhere in the neighborhood of 25 million. It wasn't doing what we wanted it to do. What the intended behavior for a counter should be. If I increment it 20 times, its count should be 21 and done. We didn't see that with the multi-threaded program. So does anyone have an idea? Are any ideas of how we could have we could fix this. Yeah, cool. So threads have own local counter and they just increment their own counter. It's not a shared resource. So everyone just increments it. And then at the end, when all of the threads are done, we just accumulate the values. So I just take all of the values that each thread came up with and sum them together. So this is really nice, right? Like, well, there are trade-offs. It's like everything. Yeah. So if you can do you can do k operations in parallel. All of the operations take the same amount of time. The individual operations have the same latency. Then, right? You can only get, in theory a k fold increase in the performance of the k fold increase in the latency. So your, your complexity might look something like if you have, I don't know, you're, you're running time of your algorithm is 0 of n squared. You can parallelize it perfectly. Then if we have K cores, we could do something like maybe 0 of n squared over k, where k is the number of cores that I can throw at the problem. So we can put that in the asymptotic number. But if you have a fixed constant number of cores, it doesn't change the asymptotic complexity. But sometimes reasoning about multi-threaded programs actually brings you to a solution that does have a faster running time. Not because of the parallelism, but because you're reasoning about the problem differently. Nice, Cool. So yeah, we want every increment count so we could just give everyone their own counter. And at the end, everything works. So this is what I call the easy solution. Each thread stores local count. Threads run until they're done. Then we aggregate the local counts when the threads terminate. So why might this not be sufficient? Why might there be a case where we can't just do this? Yeah. I know. Yeah. Yeah. So there is this issue of efficiency, right? Like if I have k core is now I need k counters, whereas before I only had one counter. So I am using more resources. A counter is just an int or a long or something like that. So it's probably not a huge deal to have an extra long associated with each a thread. But yeah, what are there any problems that you might be using a counter for that? Where you couldn't do this? Yeah. Nice. Yeah. So it could be that I have a problem that's ongoing right? Where I don't have an end. It can't wait for the threads to terminate. So here's like a, like a concrete example. Let's say I want to compute some statistics on a Twitter feed, not an individual's Twitter feed, but like all tweets that are happening, I want to count the number of occurrences of some word or phrase or hashtag. Right? This is not a program that I can wait until Twitter stops existing and then count the, well, maybe we could wait that long. Who knows? But like I can't just wait for the program to finish. I want to know how many count. What's what's my word count now? What's my word count in an hour? What's my word count in a month? So I could like maybe modify this so that every, every thread, everyone that's like reading these different partitions of the Twitter feed and give me their different values. But I might need to interact with the counts before threads are done. Or maybe what one thread does depends on the total count of everyone up to that point. So the programs, so it could be. So here are two possibilities, two reasons, there are many others. But one is that we might need intermediate counts. And another thing is that, right? Like this might be ongoing, so accounts could be ongoing. So there's no, no termination of the threads. And maybe different threads need to see the counts at different times. I see count during their execution. So there's a lot of reasons that this, If we just need a one-shot counter bunch of things and report the answer, we can just give everyone a local counter. But for some applications that might not be enough. So we'll return to this question next week with this problem that's called mutual exclusion. So we'll understand ways that we can. Kind of lock this counter so that only one thread has access to the counter at a time to modify it. When it's done doing its increments, it's done doing its modification. It'll unlock it, allowing another thread to do it. This is a huge, This is one of the fundamental problems in concurrent programming, is the mutual exclusion problem. We could spend months working on this one problem alone, but we'll give it at about two lectures instead. Okay? But I do want to go back to the easy solution because it's really nice. And it's an example of what are called embarrassingly parallel problems. Again, here's the contexts. So for my counter, let's say that I have a one-shot program. I just want to count a bunch of things. I know when I'm done and when I'm done, I just want to accumulate the answers. So this is an example of an embarrassingly parallel problem. So we say that a problem is embarrassingly parallel if it can be broken into many simple computations, almost all of which can be performed in parallel. So an example of this would be my local counters. I want to count up to 1 billion and I have ten threads that I can do that. I just say thread one, you take a tenth of the work thread to, you take a tenth of the work thread, three, you take attempted the work and so on. So I take my big problem, problem. I partitioned it into a bunch of pieces that can all be performed in parallel. And I give each of these to a different thread. These are my threads. Thread k. They all do their things in parallel. And then I merge the solutions together somehow. And the idea is that doing these individual parts of the problem is going to be where the bulk of the work is doing. And then once I have the solutions to these individual parts, merging the solutions back together as relatively quick. So that's when a problem is embarrassingly parallel is when I can break it down like this. So counting to 1 billion is that way. I count two, however high I need to count individually. You partition the counts up and then we just add everything together at the end. Okay, So in the last 5 min, I want to give you the example of an embarrassingly parallel problem that you'll be working on for this homework assignment. And so we want to start with this formula from high school. So I drew a circle inside of a box. And the area of the circle, something that I was forced to memorize at some point in my life, is that the area is Pi r squared. So what is r? R is the radius of the circle, half of it's diameter. The diameter of this box is two pi is this mathematical constant that shows up everywhere at 3.1, 415-92-6535 and so on. We want to like this is in some sense the definition of the value pi. But maybe, maybe I'm slipped up. Maybe I got some digits wrong here. I want to estimate this value of pi. So that's gonna be the goal of this assignment. So here's the strategy of the so-called Monte Carlo method. So the idea is to pick a random process to generate some random numbers and use those numbers in some clever way to estimate the value that we're looking for. So here's the experiment that we're running is I throw a bunch of darts at this square. Some of them are going to hit the circle. Some of them are going to miss the circle. But I'm just throwing darts, random numbers or random points in here. And the idea is that I can compute the area of the square. Then from this formula from high school, I know what the area of the circle is. So what's the area of the square? So this was two are the diameter of the square. What's its area for r squared? So the area of the square is for R-squared. Then what's the area of the circle from last from the previous slide? It's high R-squared. If I throw a dart at random at the square, the probability that it hits the circle is the ratio of areas. So the probability that a dart hits the circle is the area of the circle over the area of the square Pi r squared divided by four R squared. We can cancel out the R-squared. So we get pi over four. The idea of the Monte Carlo method is that I can estimate pi over four by throwing a bunch of darts, generating a bunch of random numbers, generating a bunch of random points, seeing the proportion of them that hit the circle. And that proportion is gonna be about Pi over four. And the more darts I throw, the closer I expect that value to be two Pi over four. So why is this process embarrassingly parallel? How, if I had 100 people, how could I do this experiment 100 times faster? Let's say I want to throw 1 billion darts. Yeah, Beautiful. Yeah, so everyone can have a fixed number of darts that they throw. They throw the darts. They tell me how many darts they threw. Me. Tell me how many of them hit the circle or landed within the circle. And then I just take all of those in aggregate them. I sum up the total number of darts thrown, I sum up the total number of hits, and I get a better estimate of pi much faster. I guess the same estimate faster. Then if I just asked a single person to throw a bunch of darts. So that's exactly what your first program is going to do. And this is again an example of something that is embarrassingly parallel. Because if I have 100 people, I can throw 1 billion darts 100 times faster because everyone just throws 10 million darts. So that's again the idea that the details are in the write-up on Monte Carlo method and the details for implementation or in the notes on multithreading. So that's again the experiment. And hopefully you'll see that if you have four cores on your computer and you run this with four threads, your program will run four times as fast. And yeah, so that's the plan. So what do we do next time? Next time we'll talk about problems that are not embarrassingly parallel. And how much of a performance increase we can expect to see in general for something that's more dependencies between the operations in this. Okay, so thanks everyone for coming out. And I will see you all on Friday. Oh, awesome. Yeah. If you have your activity, please, you can just leave it up here with me. That a question. Yeah.