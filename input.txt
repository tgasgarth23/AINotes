All right, folks, let's get started. How's everyone doing? Everyone enjoy the break, get a chance to decompress a little, I hope. All right, cool. Awesome. Yeah. So plan for today is to kind of introduce a new topic of concurrent objects. And this will take us really through most of the rest of the semester because it's talking about how we can get concurrency and parallelism with more sophisticated data structures. So, so far, we've looked at really simple data structures. We've looked, well, not even data structures, but just accessing memory in a way that's safe to use with multiple threads. So we had this example of a counter that we used over and over again. And we just had a single int that different threads needed to be able to access. So now we want to be able to talk about not just like a single memory location that we need to protect, but bigger data structures like we want to implement a linked list, we want to implement a queue. We want to implement a balanced binary tree, something like that. So how can we do that in a way that supports multithreading and is still guaranteed to be correct. There's a lot that goes into this, like even just defining what it means to be correct, is now a more challenging problem. And there's no single definition for what it means for a cue to be acute or acute implementation to be a. Q. And so that'll kind of yeah, it'll take us a little bit of time to kind of plot out the terrain and see what can happen here. Yeah. The sign different brands and different yeah. So we wanted to do something like we want to have a shared queue that acts like a queue. Multiple threads can access concurrently, but it's not. Yeah, we need to define what it means to be a q in this contexts now. But before we jump into that, a couple of announcements. First of all, lab three is due on Friday. This is the Mandel, Mandelbrot computations using vector operations. So this was what we spent most of last week talking about. One thing that I would strongly recommend that you do today if you haven't already is make sure that on your personal computer you have a version of Java that supports these vector operations. So the way to do that is just to try to compile something that uses this incubator, JDK incubator vector. And so I think if you have anything that's newer than Java 16, I think it's 16 plus. It'll let you do this. If you have an older version of Java, won't, you'll have to upgrade. So make sure that works if you run into trouble, let me know and I'll point you towards resources to update. You can also do all of this on the cluster. So even if you don't have it running on your computer, you can log into the cluster. The one thing that you do need to do before you can do this Java add modules business, you have to load the right version of Java into the cluster, because the cluster has a few different versions of Java installed. So in order to use Java 19, if you just type this command module load and then this stuff, the entire session, we'll use Java 19 whenever you use Java C or Java on the cluster. So yeah, so you can run everything on the cluster, test it that way. But yeah, if you want it on your own machine, you have to make sure that you have to recent version of Java. So any questions about the assignment or any of that before moving on? Alright, cool. So then, yeah, I also just wanted to give a heads up of what to expect for the coming week. So we're kind of, I guess at the midpoint of the semester. So kind of the main work that'll be coming, coming up. So I'll assign URL, I'll give a description of the final projects. So basically what the final project is is, well, there's three options, and you get to pick two of them that you wanna do. Um, so there's, there's kind of, yeah. So the first option is sorting. So you can write a program that sorts as quickly as possible. So this is very fundamental task, something that you can do a lot with parallelism in order to sort a list of numbers very quickly. Another option is computing prime numbers. So I'll give you a baseline implementation of a procedure that will compute a list of prime numbers. And the task that you will have for this assignment is, I will give you 1 s of compute time. You need to compute as many primes in the sequence as possible. So you have to write the list 235711 and so on. It needs to be sequential in that order. I will give you one seconds of compute time on the HPC cluster. And the measure of performance is how many sequential primacy, right? So I'll just write an array. The first number that I see that is not the next prime number. I stop that. As far as your program gets in a second, is the measure of performance of your if your implementation. So again, this is something that will require a bit more thought in terms of how to implement it. And then the third option is choose your own adventure. Which is you come up with a problem that you want to tackle, that you can employ parallelism and you'll test and do all that. So each of these things, yeah. I mean, what I what I expect of them is that you're not just sit down and apply a couple of things, but you do fairly exhaustive testing and you record the changes that you made and what sort of improvements or non improvements that you see in terms of your programs performance. And so you'll want to, the assignment or the final project. You'll, you'll do two different submissions. So you'll pick two of these, three. For the sorting and the prime numbers. There'll be a semi competitive aspect to it where I'll maintain an anonymized leaderboard of the fastest benchmarks on these. And the groups that have the fastest performance will get some extra credit or something. So yeah, so keep a heads-up for I'll post in a more official announcement about what to expect from those. So I think we'll only have one more written assignment, so that'll be due in two weeks or so. And then also one thing that was on the syllabus that we haven't done yet because I wanted to wait until we had just had more time to let the, the material in the class kind of absorb a little more, is they will have some quizzes. So we won't have a final exam in the class. But instead, you'll have every week like a ten or 15 minute quiz that will cover a topic from the more conceptual side. So not the programming side, but again, I'll, I'll give more concretely announcements about that. So you'll have plenty of plenty of time to prepare and you'll know more or less exactly what material will be covered on each quiz. Okay? So that's administrative stuff. Any questions? Alright, cool. Current objects. So what have we seen so far? So far, we've, we've looked at locks. So these are ways that we can maintain that only a single thread at a time has access to some resource. So we have our critical section of code, the thing that we want to protect. And here's like a kind of pseudo implementation of a shared counter. So we have this counter objects that if I call increments 17 times, even if different threads call it, I want to maintain that what the correct behavior should be is that at the end of the execution count stores the value 17 because it was incremented 17 times. So here's what we could do in order to get that behavior. So we have our count variable that stores just the value that we want to to the store. And then we have a lock inside of our shared counter as well. So what this lock does again is that if multiple threads concurrently call lock, only one can return before the, before another one or before anyone unlocks. So the way that we increment the counter with a lock is we first we try to obtain the lock and we're stuck here until we do obtain the lock. Once we return from this loc method, we have the lock, we have exclusive access to. The critical section. So we do our computation in the critical section which is incrementing the value. And then when I'm done incrementing the value, I release the lock, so I call the unlock method. This will guarantee that the, the counter does what we expect to counter to do. Because it essentially forces the execution to be sequential. Whoever gets the lock first does their increment, and the next person can't increment until they're finished. The first person's done with their increment. So that's the idea and what we've done so far. So what are the, I don't know if we can kind of do this with any object that we might want to have concurrent access to. So like what are the advantages or disadvantages of this approach? In particular, in terms of reasoning about the correctness of the procedure and also thinking about the performance of the procedure. What does locking bias, what does it cost us? So what did we, what was the problem? I guess let's maybe focus on correctness first. What was the issue with correctness when we didn't have locks with our counter object. Yeah. Nice. Yeah, So on the correctness size we decide we get this mutual exclusion property that says that only one thread is ever in the critical section at a time. We can just place an order on the accesses to the critical section and say there is a first access by the first threat that obtains the lock. There is a second access where the second threat obtains the law. And because of the locking and unlocking, we get we get that these accesses are separated from each other and so they can be ordered. And so are our execution is essentially sequential. On the other hand. So that makes it easy to reason about the correctness. Because if our sequential implementation is correct, then when we add the lock, the concurrent implementation is also correct because it does the same thing as the sequential execution. But that harms us in terms of performance. Because now we can no longer do multiple things in parallelism, in parallel. So we lose parallelism. So all of this stuff that we've been looking at in class so far in terms of improving performance, we lose all of that in the name of getting something that works. And so usually with a program, it's not the case that all of it needs to be like every part, every computation needs to be performed sequentially. And so we still can get good performance boosts if we're only accessing this shared object infrequently. But we don't get this kind of this free speedup that we might by just throwing more threads at a problem. Like the early examples such as like computing pi or something like that. So we get easy to reason about correctness. If we have a sequential, the correct implementation with locks, we get the same guarantee, but we lose out on parallelism. Yeah, sorry. Whoops, I meant to erase. Same. Alright, so these are the advantages and disadvantages of using the law. So again, the idea is if you are only incrementing the counter, say very infrequently. The sequential slowdown doesn't necessarily harm us. But in some cases we might want to do something more sophisticated. So today what we'll do is we'll talk about things that are more complicated than just a single integer value that we want to protect. The main questions. So concurrent ADT, ADT is abstract datatype. So something like a cue. List, a stack, whatever you have, something that specifies what responses you get from a sequence of operations. Okay, so the main questions we want to address are, how can we guarantee that our program is correct if we allow concurrent access? So again, this might be easy with locks. But again, the tradeoff is that if we just kinda throw a lock at a data structure, maybe we no longer get any performance increased from it, or maybe we really slowed down the, the the performance. And again, there's this question of like, what do I even mean by correctness now? Like, what does it mean to be a q when you can have concurrent enqueue operations and concurrent dequeue operations. Then the second question is, how can we achieve best multi-threaded performance? So this is where the locks potentially harm us because they can just essentially forced to sequential execution on us. When what we want is to get some performance boost from parallelism. That's where we wanna go. So let's think of a first example of a data structure that most people encounter. Maybe the after an array or a linked list is the next data structure that you implement. I mean, who implements an array? This is maybe the first data structure that you implement by hand. What is a doubly-linked lists look like? So we have a bunch of nodes. This is a node and it stores references to the case of a doubly-linked list of previous node, previous node. And it's next node. Each node has a previous neighbour and the next neighbor. Now we want to think about, I have a linked list that multiple threads are accessing at the same time. So let's say I just wanted to do an insertion single thread. This is what we, we've all done before. So here's my link list. So I want to insert this new node between these other nodes. So here's the new node to insert. And then here's the location that I want to do the insertion. So I want to do it after this node. How do I do that? Well, I just update the previous and next fields of everyone involved. So who did I need to modify in order to do this insertion? All three of these nodes kind of got modified, modify the previous, next of these three. So that's how it looks in pictures. So what does kind of like a, a computer science one-twelfth style implementation of this look like. We have a node class stores next to previous and some value associated with the node. Presumably we're storing something in our linked list. Then how do we do our insert? So I want to insert, so what's this method doing? So I want to insert after the node in D. So insert node with value after Andy. So what do I do? I look for the next node after n d. I create a new node to store my new value. I set these next two point to the new, the new node. I set the new node's previous 2.2 and D. I've set the next, the current nodes are the new node's next to point to these next. If Andy's next is not null, I said its previous pointer to point to me. So we're just doing this insertion in code. So everyone happy with this? I guess this is like hopefully familiar. But that's one thread. So what could happen if I have two threads? So maybe I have say, T1 calls insert and d, ten and thread two calls insert, same ND and the value nine. So this happened concurrently. So what might happen to my, my linked list? So here's Andy. So maybe I have my next here. So before I had this picture, what's going to happen when these two threads come along and try to concurrently call insert or what could happen. Yeah. Yeah, so right. Like this is the first place where bad stuff can happen, right? So let's say both of the threads make it that far. So thread one made, made it occur for thread one. And maybe thread two comes along and makes its Kerr for thread two. So now I have these two new nodes that are created after the first two lines get executed. Then the next line determines what could happen, right? So ND dot next either goes to, is gonna get set to one of these two. Then the next one's going to override it. So now we could have either of these happening. So we could either have ND DOT next points to here, or we could have ND DOT next points to here. And this is happening at lime. Lime, I don't know. We'll call this star. That's making them point to, right? If you're doing an insertion, you need to modify two nodes, the node before you, in the node after you. So the first line is just getting the node that will be after you. Like the insert method is like insert after node and D. But in order to do that, I need to know what, what previously came after MD. Okay. So lines star, I have an issue already, right? Where the next of n d is either going to point to T1s current or T2s current. So in the best-case, one of only one of these is going to get inserted at this point. Because what does it mean to be inserted into the list? It means that starting from the head of the list, I follow the next pointers and I eventually reach your, your notes that you just inserted. Then we can have some other really bad stuff happens. So Current, previous and current next. So these lines are fine because they're only updating current. And no matter what everything is, we're only updating the threads own, own node that it created. But then we also have our second kind of danger zone here. Maybe this is double star. Is that I could have next previous pointing either to T1 or T2 after the execution. These happen with double star. So depending on how this all pans out, right? It could be the case that like, so, it could be the case that node or NDAs next points to T1s current. And it could be that next. So this red nose, previous points to T2s current. So we get this weird break in our, in our linked list. So it could be that if I traverse the list from left to right, I see the current that was added by T1. But if I traverse the list from right to left, I'll see you the current that was added by T2. It will never be the case that I see both of these elements added in. What I would want probably is that if two threads try to add something to the linked list, they both get added. But with this implementation, without any safeguards, It's possible that that one doesn't get added or they get added in some weird way. And maybe it's possible that what I intend to actually does happen, maybe T1 does all of its steps and inserts itself. And then T2 just gets inserted before T1s current. So this is a mess though, right? Like so this, this doesn't, doesn't guarantee that either node is inserted properly. This is bad news. Okay? So how could we fix the problem? Would be one way that we could make it make everything work the way that this would have worked. And we just done like a computer science one-twelfth style implementation. Yeah, So luck. So what should I lock? Where? So yeah, I could try to lock. So the two bad statements were when I was modifying ND and when I was modifying Andy's next. So I could surround each of these Beilock. I mean, is that going to fix all of our problems? It's not totally obvious to me that this would fix the problem. So it would fix the problem where, right, Like where node's next or previous wins out. But the issue still remains from this example before, where maybe thread one obtains next or previous lock first. So then the next node comes in and overwrites that. And then maybe the other thread winds for the second lock. So that thread, when it calls this line, it sets next previous. This gets overwritten. If we just surround these two statements by locks, we could still have this weird behavior where we end up with ND. So Kerr, I'll come car1 and Kurt two. Next. We could still end up with a case where say ND is next points to occur, occurs next points to next. But next previous points to occur. And CORS cartoons previous points to ND. So if we just lock these two lines of code, we can still run into the same issue that we had before. Yeah, nice. So how about if we lock the whole thing? Lock the whole list. Yeah, it makes the multithreading pointless, but it fix it. But at least we still have a linked list now, right? Because performance is, is useless if it doesn't perform the task that you want it to. Our highest priority is always correctness. We always want a linked list. If we're implementing a linked list, we want every operation to do what we expect that operation to do. Performance is always secondary to correctness. So we can prioritize correctness and make something that's reasonably easy to reason about. We could just lock the whole list. I just add to my linked list a lock. And now in order to do anything all of these operations, I have to obtain the lock first. So this whole thing is my critical section. So what happens now if two threads concurrently call, call, insert. So I have Kerr, I have next. I have, oops, sorry. That shouldn't be cardiac should be previous. Then I have current one in blue to red. So what happens if say kirwan obtains the lock first? Yeah. That's nice. Yeah. Right. I should use consistent naming. So previous, this is Md. Yeah, so now car1 does its insertion. It succeeds because again, it's the only one that has access to the critical section. Then cartoon comes along. It's still pointing to ND, but MDs next is now pointing to one. So these will get overwritten. Andy's next will point to Kurt two, and so on. Now at the end, my linked list goes ND, cartoon one next. So we're just resolving this concurrent call to insert by essentially making it two separate sequential calls. Whoever calls first and whoever called second based on whoever obtains the lock first and whoever obtains the lock second. If my implementation, my sequential implementation was correct. Now my concurrent implementation is also correct because the locking essentially forces the concurrent execution to act like a sequential execution. You all think, are we happy with this? Is this the best we could do? This is what my locked execution looks like if I want to insert, say, in two different locations. So red tries to do the first insertion, blue tries to do the second insertion. So someone acquires the lock first, say it's the red. They perform their insertion. They released the lock. That leaves blue free to obtain the lock. And they do their insertion. They released the lock and everything's good. So again, we're just turning this concurrent execution into a sequential one. Nice. But could we have done this faster with this picture? What would be a natural concurrent thing to try to do if I had these two insertions together? Yeah. Surgeries at different places. Yeah. Right. So if the yeah. So the the idea is that if my my insertions don't don't interact with each other on separate, they only modify separate nodes. Maybe we should be able to do them in parallel. So can insert nodes in parallel if no overlapping modifications of nodes. So as I have it pictured here, like the red insertion only is going to modify these two nodes. And the blue insertions only going to modify these two nodes. So maybe I should do, be able to do something more clever than just locking the whole data structure, enforcing blue to wait until read finishes. So what would that look like? Yeah, when could we insert these concurrently? Like, what's the condition that I need to check to see if two insertions could be done concurrently or in parallel, I guess I should say. So. Could I do concurrent insertions here and here? Well, so who does the left insertion modify? That one? Wants to modify both of these. And then which does the right one want to modify the one after, it's fine. But here we have a problem. I can't do those two insertions concurrently. But I could e.g. insert here and here concurrently, right. So these are okay. What's the condition in this case? Yeah. Yeah. So they don't want to modify any of the same nodes. So yeah, like on the left here, I have my two nodes that I want to modify. And on the right here I have two nodes that I want to modify. They don't overlap. So I'm fine modifying both of them at the same time. The one-hour mark. So you could do sorry. Which so let me 123456. So which ones? It could have been that the time to insert into that secondary. Into this position, 4-5? Yeah. Five, 3-2. Yep. So then these two we could also do together because they both want to modify different, only different notes already in the list. Okay, So if we lock the whole data structure, like this is bad news. But now we could try to do a more fine-grained locking, right? So instead of locking the entire data structure, what if we just tried to lock individual nodes? Not the whole list? So maybe we just lock individual nodes. So if I'm going to modify two nodes, I need to obtain both of their locks before I can proceed with the modification. So how does this look? So which nodes need to be locked? The one before and the one after. So the red node would need to lock this. And this blue node or the blue thread needs to lock this and this red and blue. Well, so we'll have to change our node class so that it supports this. And so we'll just have to wait. When I tried to do an insertion, I have to obtain two locks now instead of one. And then I wait until I've obtained both of those locks. And then I can do the insert method once I have both locks. Yeah, What does this look like in code? Now my node before I had Previous Next and value. So what do I add to it? I had a lock, and now I have a method that allows me to lock and unlock the node. So if I try to lock a node, I need to obtain its lock. To unlock it, I just released its lock. This is all I changed to my node class. How do I do my insertion? So I want to insert after node and D. So I obtain the lock for ND. Then after I've obtained that lock, I go to its next node. And if it's next node is not null. So if I'm not inserting at the very end of the list that I need to lock the next node as well. Then once I've obtained both locks, I can do my actual modifications and insert everything. Then finally, when I'm done doing that, I released both locks. So how does this look in pictures? So in this picture, red can acquire both of the red nodes locks. Blue can require both of the blue nodes locks. They both do their insertion in parallel potentially in both release. And so now I've done two insertions in the time that it would ordinarily take me to do one insertion. Whether or not I can do this depends on where I'm doing the insertion. But if my list is big and maybe my insertions are all over the place, maybe most of the time there isn't any contention and all of this works fine. And so I actually do get a performance speedup from this. What happens if there is contention? So now they both want to obtain the lock for this node, right? So what happens in this case, or what could happen? So exactly one of them is gonna get access at that at a time. And I don't know which one. It could be the red one. It could be the blue one. But maybe the red one acquires that lock first. Now the blue node needs to wait. And it's going to wait until the red node finishes or the red thread finishes. So maybe the red finishes. Then finally, it releases its lock. Now blue can obtain the locks and do its insertion. So in this case, we do, we do end up with the sequential execution. But It's still, is inserting both of the nodes the way we would have expected in a sequential execution. We don't, we no longer determined the order in which these items are added in like real wall clock time. But the insertion looks as though I had done one and then done the next. And that's exactly what we want our LinkedList to do. Yeah. So do something like this. Say, well, the blue, the red nodes actually only going to change the previous of this node and the blue nodes only going to change the next. So maybe we can actually do these in parallel. So how would I, what would my implementation has to look like in order to support that kind of parallelism. We can absolutely do that. So, right, like in this case I was unlocking, let's go back. Locking the entire node. I have one lock for the, the whole node. And if anyone obtains the lock, no one can modify anything for that node. So it could just as easily have a lock for the previous value and lock for the next value. So I actually have two locks and my nodes. Now, if in this case that you have here, read only wants to modify previous and blue only wants to modify next. So we could have it that the red thread gets the previous lock and this node and the blue thread gets the next not lock on this node. And then we can do that in parallel. So this is absolutely something we could do. Whether or not it's worth it. We'd have to look at the application because, right, the cost is now I have two locks, so my node class is much heavier. There's a lot more data associated with it then before. But if there's a lot of these operations going on, maybe it's worthwhile for me to do that. So yeah, you're absolutely right. Like we can decide what level of granularity we want to look at this. I think this operation is fine, but then we'll have to think about like, what happens if another third thread comes along and tries to do an insertion here and tries to get the next. You have to make sure that everything is done in the right order. Absolutely. This is a great idea. Okay, cool. So this seems pretty good. Is everyone happy with it? All right. What if we have a circular linked list? These show up in a lot of applications where you don't have a head and a tail of your list, but you actually just go around in a circle. Maybe I want to do the same thing there. So think about what could happen. So what happens? We have multiple concurrent insertions. They all guaranteed to succeed. Let's say I have a thread that comes along and wants to insert. So here's T1. Wants to insert 1-2. Another thread, T2 comes along, inserts 2-3, something like that. What? Maybe we can keep going right? More threads. So you have potentially we could lock every node. So maybe T1 gets the lock for node one, and t2 gets the lock for node two. So T1 is stuck. It can't do anything because it doesn't have to lock for T2. T2 just got the lock for, for node two. But what else could happen if I have more threads here? Yeah, nice. So maybe T3 gets this lock. Now I'm running out of colors. So let's cycle back around. T4, gets this lock. T5 gets this lock, right? So if I'm trying to do five concurrent insertions, now, everyone stores are holds the lock for, for one node. And P1 is waiting on T2. T2 is waiting on T3, T3 is waiting on T4, T4 is waiting on T5, and T5 is waiting on T1. So what happens now? We're stuck. So even though my locs are fine, they guarantee that someone makes progress. Whoever asks for the lock, someone obtains it. Even though all of my locs have say starvation freedom are deadlock freedom. Because I have more than one lock on my data structure. These locks can interact with one another in interesting ways. If I have five threads trying to all obtain five locks on this data structure, they can block each other. This is called a blocking execution. So even though no one is blocked from obtaining a single a single lock, like all of the locks individually are well-behaved because we have these dependencies between the locks on how we access the larger data structure. I'm no longer guaranteed that anyone can do an insertion. So this is the downside of locking is that if I do locking of the entire data structure, it slows me down because only one person can modify the data structure at a time. If I do locking of individual components of the data structure, I may be able to do things in parallel. I may get a speedup. But if I'm not really careful, we can run into or allow cases like this, where individual threads are doing the right thing. Individual locks are doing the right thing. But collectively we get stuck. Everyone's waiting on someone else. And the more locks we have, the worst this problem gets, the harder it becomes to to mediate between these. Yeah. Maybe, but then we need to change what our locks are doing. Right. Like we need a fundamentally different type of lock than the locks we've looked at so far. So this is a fundamental tension, is that locking the whole object like a linked list is easy to reason about why it works. Because it essentially forces us into a sequential execution. But because it gives us a sequential execution, we no longer can get any value from parallelism. On the other hand, locking individual parts might allow us to perform more operations in parallel and therefore give better performance. But we have to be really careful that we don't allow for things to get stuck. This is the main tension that we have and something that we'll explore quite a bit in the coming weeks. Alright, so that's everything I wanted to say for today. So we'll follow it up on Wednesday. We'll talk about what it even means to guarantee correctness in terms of people don't, or threads don't get stuck. And also just like what it even means to be a stack or a queue or something that allows for multiple concurrent accesses, right? Thanks everyone for coming out and I will see you on Wednesday.