Today. All right, Cool. Has everyone doing? Alright, cool. So yeah, I'm super excited about today's class because we're talking about sorting networks. This is one of my favorite topics. In, at least in this class. We won't have time to do it, complete justice. But we'll, we'll scratch the surface and think about sorting in a different way than we have so far. So first of all, just a couple of quick announcements. So I had mentioned for the final projects, you'll have these weekly check-in assignments are not assignments, but just so you can upload whatever your current snapshot is, your progress on on the, the sorting and the primes assignment. So I haven't made an up like a submission link yet. I'll do that today. So I'll run everything on Sunday. So as long as you submit it before then I can I'll update the leaderboard. I'm not going to grade you on this one, so this weeks is optional. Next week you should absolutely submit your stuff. Also, part of what they want to push back the deadline, not today, is that it's really beautiful out. And if you have a chance to just like go walk around this afternoon for an hour instead of sitting in front of your computer. I encourage you to do that because it looks like it's going to get colder and wetter over the weekend and early next week. So enjoys a sudden this afternoon, if you can. Also, yeah, you should submit your Choose Your Own Adventure baseline if you're doing a project other than sorting and primes. So that submission link will also be open later today and I'll send out an e-mail reminding everyone. Okay. That's all I wanted to say about administrative stuff. Because I want to talk about sorting small arrays. So usually when we talk about the efficiency of algorithms in computer science, we're talking about asymptotic efficiency, like as your input gets larger, how efficient is your procedure? In reality, that's not always the best measure of performance. It gives you kind of course, measure of how good your algorithm is in terms of how it scales with the input size. But when it, when it comes down to implementation, oftentimes we care about doing the small, frequently performed tasks as efficiently as possible. And so that's going to be the focus of today is rather than sorting an array of size 1 million, I want you to sort an array of size eight is quickly as possible. So let's look at an elementary sorting algorithms. So this is Java code for insertion sorting. So how does insertion sort work? Well, here's the code for it. What does it mean? You have an array of elements and you want to sort them? And how does it start off? So it starts off by we look at the first two elements. And if the second element is smaller than the first element, we swap them. That's our first iteration for I equals one. Our next iteration, what do we do? We start at index two. And we see, is that bigger than what's an index one? If so, we swap it and then we go down to index one. We see if what's an index one is larger than or smaller than what's in index zero after I did the previous swap. And I swap them. So we can just continue this procedure and so did an intermediate step. What's the idea here is that if I go up to iteration, I hear the invariant that this algorithm gives us is that everything up to index I is sorted. The sequence of swaps performed by the inner loop. The inner for loop will take whatever entry is here and swap it down until it appears in sorted place with the remaining elements. So that's the interpretation of insertion sort. It's kind of one of the classic fundamental elementary sorting algorithms where it doesn't require any like divide and conquer cleverness or anything like that. It is just the first I elements. And then to sort the I plus first element, just drag it down into sorted, into its sorted order with the previous elements. And then you have first I plus one elements sorted. So in terms of sorting larger raise, this procedure becomes inefficient because it's runtime is we can only bounded by 0 of n squared. So we have faster algorithms like merge sort, which is 0 of n log n. And in practice, quicksort tends to be much faster as well. So this is our elementary sort. But if you're sorting small arrays and you implement this, this is actually one of the fastest algorithms, or at least the fastest algorithm that I've ever tested. For these kind of elementary sorting algorithms for small arrays. It's faster than merge sort, it's faster than quicksort up to size like 100 or so. So cool. This is insertion sort. I want to think about it in a different way. So first of all, what are what are its appealing features? One is that the only modifications that the, that the procedure ever makes to the array is in this if statement here, which just considers two adjacent elements in my array. And if they're out of place, I swap them. That's the only modification and the algorithm is just repetitions of this. So what's interesting about this is that the access pattern, the order in which I do these operations, doesn't depend on the initial array. It's always the same. I always start at indices 0.1. Then in my next iteration I always do 1.2, then 0.1. And my next iteration I always do 2.3, 1.2, and 0.1. I always perform the same operations on the same locations of my array regardless of what the values of the array r. So this is something that's actually really nice because the memory access pattern is very predictable. And this is why, or at least part of why. This is such an efficient algorithm in practice for small arrays. So these are two features of insertion sort that are nice. So it only modifies adjacent elements in the array. And the access pattern is always the same whether or not I do the swap differs from, from array to array. But it's always the same order in which I'm accessing the array. So I wanna give you another way of thinking about insertion sort. And that's by what's called a sorting network. So our primitive operation in, in performing, in doing, implementing this algorithm is just I have two values in my array. I compare them and if they're out-of-order, I swap them. Otherwise I don't modify the array. I'm going to depict this operation as follows. So here's an index of my array. Maybe this is like AI. Here's another index, AJ. And at the end, I want to store the minimum of these two values. So minimum x and y. And I want a j to store the maximum of the two values. So that's what this one little line of code is accomplishing. And I'm going to depict that code is just this gadget. I have a wire for each value, so I call this a wire. Wires a horizontal line. And I'm going to draw a vertical line between them with dots. To say that I'm doing a comparator, the comparator operation. So what the interpretation of this picture is is that I have a value X that comes along the wire. I have a value y that comes along the wire. The comparator compares them and if they're out of order, it swaps them. So the, whatever the smaller of the two values is, we'll end up at the top wire. Whatever the larger of the two values is, we'll end up at the bottom wire. That's what a comparator does. It's just a gadget that compares two values and spits out the larger, the larger value on the bottom and the smaller value on top. What does this look like? Just an examples. So here's, if I was storing the value three on top and two on the bottom, the comparator would swap them, it would do would move up to the top and three would move down to the bottom. If I had a comparator with the values five on top and seven on the bottom. Five smaller than seven, they're in order, so the comparator wouldn't swap them. Five stays on top, seven stays on the bottom. So if I want to sort an array of two elements, I want the smaller value at index zero, I want the larger value at index one. This picture gives describes an algorithm for sorting an array of size two. I need one comparison, one comparator. And it just says, if data zero is smaller, keep that value in place. If data zero is larger than data one, swap them. So this is equivalent to if data zero is larger than data one. Swap data 01, so swap the two values. This picture represents sorting just two elements in an array. Any questions? So far? I'm just setting up a single elementary operation. And then we're going to combine these operations to get sort more generally. Okay, so let's look at insertion sort. So I have, now I have my basic comparator operation. And I can think of each of these lanes as being an index of my array. Index zero, index one, index two, index three. So those are the four values in my array. I'm want to sort an array of length four, which is the first comparison made by insertion sort. What are the indices being compared? So I start, I sat idle one. I go into the inner for loop. I set j to I, which is one. And then I'm doing my comparison here. What's, what am I comparing? Which two values or which two indices? On my first iteration. Nice, so I'm performing a swap or a comparison, 0-1, That's the first comparison that I'm doing. This is I equals one, J equals one. Okay? What happens in the next iteration? What's the next comparison that I make? That's the only one iteration that I do with I equals one. So then I move on to I equals two. Yeah. So then I compare two to one after that. So I'm doing this comparison. Then what's the next comparison that I make after that? So this is I equals to j equals two. Then I do, yep, I do 1.0. Again. This is my I equals to j equals one. Then what's the next comparison that I do? Yeah, beautiful. 322110. So that's four. This is my j equals three iteration. I equals three. Exact same code as before. I'm just coming up with a different way of visualizing what the code is doing. So I can take this code and kind of unwrap it, what it does on my array of size four. And I can draw this picture. So now each iteration corresponds to one of these comparator operations. Let's break it down. So insertion sort I equals one, we get this, I equals two, we get two comparisons. I equals three, we get three comparisons. So what are we doing in this class? This is a sequence of comparisons that I'm making. What's the point of this class? What's different about this class from a sequence of operations? What do we want to know next? Right? So which of these operations can I do in parallel? Which ones don't depend on the others in terms of the outcomes. So let's first look at it. Maybe these two these two comparisons, does the outcome of one depend on the outcome of the other? Well, wait, no. So what if I had sorry, what if I had 431, something like that. Right? So this first comparator, we'll swap them. So I'll have 34 after I do that comparator. And the outcome of the next comparator for will bubble down and one will come up, right? And so what the values here and here, our depend on what these two values are, which depend on, well, which will be modified by this comparator. So these two depend on each other and how could I tell that they depend on each other from the picture. Yeah, So they share a lane here, right? And more generally, there's a path from one of the inputs of one of them to one of the inputs if another one. So I could go right like I could end up with four moving down all the way through here. So since four starts at this lane and could end up in this lane. As a result of these comparisons are these comparators? These two comparators do depend on each other. So we do have dependency there. But what are some comparator operations where there aren't any dependencies between them, but they're kinda logically independent of one another. Yeah. It'd be the first one. Share yeah. The order. Well, but here's an issue. Right. Is for let's say I have two here, right? Four is going to start at this comparator. It's going to go down, it's gonna go down, and then it ends up as an input in this comparator. Because I can find a path that starts on this comparator and ends on this comparator. They do depend on each other. Yeah. So in a given row and the first and third ones, but these definitely depend on the output of the other, right? Because what ends up here depends on what, you know, the output there. Yeah, Mia, the third one, the fourth one. Nice. So if I take these two comparators, there's no way to get from an output of one of those to an input if the other. So I can do both of these operations in parallel. I don't, there's no way that the output of one of these could change the input if the other one, there's no path that goes forward along the lanes that leads from one of these to the other. So these are the only two that can be parallelized. The other ones all depend on one another. I'm going to take this picture. I'm just going to clean it up. I'm going to write it in columns where each column precedes the other one. But within a column, the operations can be performed in parallel. Yeah. Well, let's try to generalize further. Let's look at it. One bigger example. Oh, yeah. So before I do that, I want to define one term, but yeah, so let's, let's put a pin in your comment for a second because we'll return to it. Okay? So now I want to ask, right? Like I, I've separated these into these columns. And each column I can perform all of the operations in parallel, but I have to do the columns themselves sequentially. I have to do this operation first. Then I do this, this operation, 345. So I can do this in five parallel steps. Whereas the sequential code told me that I should do this in six steps. I had one iteration of the first outer loop, two iterations in the second and three iterations in the third. So I've reduced potentially five or six sequential operations to five parallel or sequential operations where each one does multiple things or can do multiple things in parallel. So I wanted to find just really quickly one other thing here, and that's the depth. The depth is defined to be the longest path from the length of, I guess, the longest path from input. So that's the left side to output, which is the right side. And what do I mean by length? The length is the number of comparators that you cross or the number of competitors that you touch. And so in this example, so you should convince yourself that the longest path is actually just taking ln two. So this is a path of length five. There's no path of length six in this, in this network. Again, we're thinking of time proceeding from left to right. So I can never move backwards along, Elaine, I only move forwards when I'm doing a path. So I don't e.g. have something silly like I start here and then I go back and then I come along and do something weird. So that's not allowed. The only thing is that I have to keep moving forward along each lane. And I just ask what's the largest number of comparators that a single that I can hit as I move along here. So that's one path of length five. So here's another path of length five would be, let me grab another color here. Would be starting here, going like this. This is also another, another path. Oh wait, no, that's not like five, is it? How many did I hit? I hit 1234. That's only for maybe, is this the unique path of length five? No, I think I can go this way. I could do one, something like that. That's another path length five because I hit F5 comparator is along my way. Okay? So another thing that you should convince yourself of is depth is equal to the minimum number of parallel steps in applying the network. Okay? So if I have depth five, I can partition my network into five phases, where within each phase, all of the operations can be performed in parallel. This is something that one could prove. I'm not going to prove it, but it is true. So depth gives us a notion of how many parallel steps does it take to sort. Okay? So questions here before we return to comment. Alright. So here's insertion sort for larger instance. This is I equals one. Again, I equals to I equals three, y equals four, I equals five. So apparently, I'm sorting an array of six elements. And these are the phases that the algorithm takes. But looking at this network, which of these operations can I do in parallel? So as before, we had, right, I could do this operation and this operation in parallel. What other operations might I be able to do in parallel? Let's go. I don't know. I'll label the phases 12345. And then you just tell me within each phase which which which item you want to talk about? Yeah. The first one and the third, the first and the third. And then last in the fourth. Nice. So I can actually do all of these one in the last and the fourth, this one as well. So if I do that, right, so I have a path that goes from here to here to here. First on top. Okay. So like this one and the bottom one here. Cool. Yeah, so there's no way to get from one of those to the other. So I can do those in parallel. Any other operations? Yeah. I think still using Uh-huh. Awesome. Okay. Nice. So we could do this one and this one, right? Because of that necessitates to also match that. So I could do these three operations in parallel, all the yellow ones. Okay. Anything else? Nice. 4.3. Oops, not that. I want a highlighter. 5.2. Awesome. Like these, the ones that are grouped together can be done in parallel. Let's clean up the picture and do everything in parallel that can be done in parallel. Alright. So what's the depth here? Yes, we have, right? Basically the way that I broke it up into columns is the best way that we could break it up into columns. And that's my depth. So we have 12 who don't like this green color, 12345678. So I can do my sorting of six elements using, using insertion sort, but now in just nine parallel steps, instead of my original algorithm would have me do 123-45-6789, 10 111-213-1415 sequential steps. So by employing parallelism, If I do these comparison and swap operations in parallel, I'm reducing the time from from 15 sequential operations to nine parallel operations and then parallel phases. Okay? Parallel depth. Cool. Any other questions? If not, let's look at another sorting algorithm, bubblesort. So bubblesort is another one of these elementary sorting algorithms that takes elements. And it's kind of like insertion sort, but in reverse, where in the first iteration I go and I take, I do a bunch of comparison swaps, comparison swap operations. And I do this all the way up until I get to the top of the array. In this guarantees after one outer iteration that the largest value is in the last index of the array. So now that I've sorted the last index, I repeat going up to be going up to the same thing, but stopping at the second-to-last index. And we just keep on doing the same thing. So that's what the code looks like. Let's look at the sorting network for this. So when m equals six, I'm bubbling up all of the elements to the last, to the end of the array. So this is the sequence of comparisons that I'm doing. The next iteration, I do the same thing, but stop at the second-to-last wire instead of the last wire. Next iteration I stopped at the third to last wire. Next iteration I stopped to the second-to-last wire. Next iteration I just do the first two comparisons. So bubblesort corresponds to this sorting network. So if I break everything, so here's my sorting network. And we can do the same trick where I can identify different operations that can be done in parallel. So e.g. I. Could do this operation and this operation in parallel. I can do, yeah, whatever. This operation, this operation, this operation in parallel. Same thing. So if I parallelized stuff, I do, yeah, so let's draw circles around them. So if I parallelize these two operations, parallelize these, which these two, these three, these two, these two. Now I can just shift everything around a little bit, draw these vertically. I get this sorting network. Does this look familiar? Yeah. Yep. So if I maximize my parallelism for insertion sort and bubble sort, I get exactly the same procedure. So bubble sort and insertion sort only different the order in which they do up in the sequential order in which they do operations. But if I parallelize everything, if I do multiple comparisons in parallel when I can, they're exactly the same procedure. So insertion sort, bubble sort. Not only does the structure of the code looks similar, but in some sense they're exactly the same procedure. When I ignore kind of the, the artificial sequential nis that's imposed on the code in writing them. So this is the same as insertion sort. So parallelized insertion sort is the same as parallelized bubblesort. Let's kinda cool. But we can do better when we just want to sort smaller things. So insertion sort and bubble sort. Both perform the same operations. They only differ in the order in which they perform the operations. But these are the differences in the ordering are don't matter if we parallelize everything. The parallel versions are reasonably efficient. So the general formula if you want to compute it, is that if I want to sort n elements, I can do this in two n minus three parallel steps. But we can do better. So think about this for a second. Maybe more than a second, a couple of minutes. How can I make them more efficient? Sorting network for four elements? What's the most kind of parallelization that I can get out of this. So give yourself a couple of minutes and try to come up with a network that sorts four elements. It's better than the network that we came up with. So what was the network that we came up with for? Whatever it was? This one. This was our sorting network of size four for insertion sort. So come up with a better network. So smaller depth, fewer parallel steps for sorting four elements. One of the slides today. So how could we do it? So let's get started together. Maybe. I have to put a comparison somewhere, right? Let's just say I'm going to compare the first two values. Yeah, what do I know here? I have one parallel step. What do I know after I've done this? Let's compare it to be cool. So yeah, it could do another I have room for another comparison. Right. So I can just put that comparison here. Yeah, so what do I know after I've done these comparisons? Well, hang on here. But before we add anything, what, what, what have I accomplished by adding these two comparators? I've done two things in parallel. And then what do I know about the values after these two things are done? So what can I say about this value versus this value? And this value versus this value. Yeah. Right. So I know that the top blue dot is smaller than the bottom blue dot. So this is the maximum of, sorry, this is the, this is the maximum of the two. And this is the minimum of the two. And this is the minimum of the two on the bottom, and this is the maximum of the two on the bottom. Alright? So what would be a sensible thing to do at this point? I know that I have the maximum of the two. I know that I have the maximum value of the first two lanes, and I know that I have the maximum value of the second two lanes. How can I find, say the maximum value of all four lanes from this? Yeah. Nice. So I can take the two Max's that I found so far and compare them. And the larger of those two is the maximum in the entire array. So how can I implement that with one comparator? Nice. So I do this, I connect these two together. And now I know that the overall maximum value is stored on the bottom wire. I've sorted that element. This is the overall max ends up here. How about doing the minimum value? How can I find the overall minimum value? Compare 123? And I can do that in parallel with my comparison of two to four, right? Because they don't depend on each other. So now I know that the overall minimum is in the right place. So what's left to do? Beautiful. Now I have two numbers in the middle and that'll sort them. This looks like merge-sort, right? Because I took the first half and I sorted it. I took the second half and I sorted it. It's not quite merge sort, but, you know, I'm kind of doing things in parallel where I'm taking the overall largest element, putting it in its right place, taking the overall smallest element, putting it into its right place. Then what's leftover is just the middle two elements and a single comparator sorts them. So at the end, I get my sorted, my force sorted values. So what's the depth of this? If this network. So it's certainly not more than four, but I did these two in parallel. I can do these two in parallel. I did this one alone. My depth is four. I can break everything up into four independent steps. My depth is, I'm sorry, 33 independent steps. So before when I did insertion sort, my depth was five. So we've made it almost twice as fast. If we can do these operations in parallel to sort four items is what insertion sort did. This is overall a better sorting network. It has fewer comparisons overall. The depth is smaller. And this is the best way to sort four elements in parallel. You can't do better than this. There's nothing that uses fewer elements. There is nothing that has smaller depth. Because this only sorts for elements like this. This network, general sorting network. We can use them for larger ones, but we don't know. The problem is that we don't know a lot, so we know what the optimal sorting networks are up to sorting 18 elements. After 18, it is an open research question. What's the best way to sort 18 or 19 elements? We don't know the answer, we don't know what the best sorting network is. Okay? We already did this. Cool. So now let's talk implementation. I want to implement this sorting network. I want to get this in my code so that I can sort for elements or some small number of elements faster than writing a program using say, insertion sort. If I want to do vectors. So now I have a vector of length four. So here's my vector, 3124, something like that. And I want to sort it 1234. So this is like Java vector. It's an array with four elements basically. How does sorting networks helped me? How could I employ this idea of a sorting network in order to perform some of these operations in parallel. So even if I just want to implement one comparator, how could I do that? Like let's say I wanna do, I wanna compare these two elements. And I want to end up with the array two, or sorry, 12211324. What would that look like in code? What could I kind of, what operations do we have for vectors that would allow us to do something like this? Alright, vector operations, the ones that we've defined so far, tend to be do the same operation to every lane. But here we're doing something fundamentally different because I'm swapping values in lanes. So guess what? Java gave us a tool to do this. So it gave us a tool called a vector shuffle. It sounds like some weird dance. But a vector shuffle is, it stores essentially an array of indices. And it just swaps around. The indices, are swapped around the values according to the index. So here's my, I had a vector that store is, sorry, I shuffle that stores indices 1032. And I have a vector that stores 5497. And I want to apply the shuffle to the vector. So I'm applying, so here's my, my vector 5497. So these are indices 0123. And then we're applying the shuffle. That is 1032. So what this does. Is this just going to rearrange the values where the first value is the previous value or the value at index zero is whatever the previous value at index one was. The value at index one is whatever the previous value of index zero was. The value at index two is the previous value at index three. The value at index, or sorry, what was the next value? The next three was the previous value at index two. Index one is this. So we end up with four on top. Index zero is five, so we end up with five. The next index, index three with a value of seven. So we end up with a seven here. Index two had a value of nine. So we end up with nine down here after we do the shuffling. Okay? So now I can swap a bunch of elements. And applying a shuffle is a single operation for a vector. So I can swap multiple elements at a time. But the problem with the comparator is that I don't swap all elements. I have to only swap if one is larger than the other, if they're out-of-order. So we can do this as well. So how could I do if I just wanted to implement? So here's my comparator. Let's do the first step of our R for comparison. So this, right. So I want to compare and swap potentially indices 0.1 and I want to compare and swap potentially indices 2.3. So what I can do is I can define the shuffle that swaps indices 0.1 and 2.3 and apply that to my vector. So let's say my vector will use the same vector as before, 54975497. And then if I do the shuffled version, shuffled, I'll get 4579. Okay. And so what I want is I want my final answer. My final to be this value at top should be the minimum of these two values. So that should be the minimum. And similarly down below, I want this value to be the minimum of these two values. And then the other ones that I want, I want this value and this value to be the maximum of their respective values. The comparative spits out a Min and max for each one. So I want the maximum, the maximum. I compute two vectors, I compute the shuffled version and the original version. And for the blue lanes, I take the minimum of the two values in these vectors at each location. And for the purple or the pink lanes, future lanes, whatever that color is, I take the maximum of the two values that I see in the corresponding vectors. We can do this in code. So almost at the end of class. So let me just jump through. And this is exactly what the code looks like for it. So I make the swapped version of the two vectors according to my shuffle. And my shuffled just encodes the right which comparisons I'm doing in parallel. And then I do a blend of the minimum of the original vector and the swapped one according to a mask. The mask tells me which lanes I'm taking the minimum of and blend just copies in the values of the updated vector of the minimum vector. And then I do the same thing for the maximum values. But now my mask, I negate because these are the lines that I want to do the maximum on. So what I'll give you two toy with a guess over the weekend is an optimal sorting network of size eight. So this is the best way of sorting eight elements. And I implemented it in Java. This is what the implementation looks like in vectors. I just define. It works in six layers. Each layer has a corresponding shuffle. So these are my six shuffles that I'm doing, that I'm going to do in parallel. And then I have the mask saying if computing a minimum or not for that lane, when I apply that shuffle, then I create a vector mask, vector shuffles from those. And then this is what my sorting algorithm actually looks like. I just have a sequence of vector operations. There's no control flow, there's no if statement, there's no branching. It is just computer. Do these operations, do all six of them in each one does eight parallel computations. Just so that I can prove to you that this is actually worth doing. Let's run it and test it. Compared to the fastest algorithm that I found sequentially for eight elements, which is insertion sort. It's a lot of code. But let's see, i'm, I'm going to sort for you. Does this fit on the screen? There we go. So I'm going to sort for you. What is it? 8 million elements are 8 million chunks of 1 million chunks of eight elements. And we'll see which is faster my very long code. Or come on. I need to draw a C or a simple less code, but something else, so it takes a little while. So sorting in, okay, let's zoom in on this. So sorting using insertion sort took one-and-a-half seconds. Sorting with the network executing all of this code took half of a second. So three times as fast to sort in this kind of complicated way compared to three times longer to sort purely sequentially. And we're only using vector operations here. You can still parallelized stuff further if you wanted to do a bunch of these operations in bulk. So sorry for keeping you a little bit long, but I wanted to jump to the punchline. So you can download this code, play with it, do all of that, and incorporate it if you see a use for it in your sorting projects. So thanks everyone for coming out. Have a nice weekend and enjoy the weather. Yeah.